{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Accuracy hovers at around 50% using the VGG-16 pretrained model, and training all the layers. It might be better fine-tuning only the last layers. However, don't expect things to get drastically better. Also, the training is taking (very) very long, i.e. ~10 minutes for a single epoch. So abandoning this for now.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import *\n",
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "from __future__ import print_function, division\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.load('x_data.npy')\n",
    "y_data = np.load('y_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load pretrained resnet.\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.classifier[6].out_features = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('cuda available')\n",
    "    vgg16 = vgg16.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that only fc parameters are being optimized\n",
    "optimizer = Adam(vgg16.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    \"\"\"Dataset wrapping data and target tensors.\n",
    "    \n",
    "    Each sample will be retrieved by indexing both tensors along the first\n",
    "    dimension.\n",
    "\n",
    "    Arguments:\n",
    "        data_tensor (Tensor): contains sample data.\n",
    "        target_tensor (Tensor): contains sample targets (labels).\n",
    "    \"\"\"\n",
    "\n",
    "    def getCategoryLabel(self, y_vec):\n",
    "        tot = y_vec.shape[0]\n",
    "        vec = y_vec.shape[1]\n",
    "\n",
    "        y_cat = np.zeros(tot)\n",
    "        for i in range(tot):\n",
    "            for j in range(vec):\n",
    "                if (y_vec[i,j] == 1.0):\n",
    "                    y_cat[i]=j\n",
    "                    break                       \n",
    "        return y_cat    \n",
    "    \n",
    "    def __init__(self, data_arr, target_arr):\n",
    "        assert data_arr.shape[0] == target_arr.shape[0]\n",
    "        self.data_arr = data_arr\n",
    "        self.target_arr = torch.from_numpy(self.getCategoryLabel(target_arr).astype('uint8'))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Scale((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])            \n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # this hack needed because transform will not act on images directly!\n",
    "        # Have to first convert to img using Image.fromarray(..)\n",
    "        # The use the transforms from above\n",
    "        temp_img = Image.fromarray(self.data_arr[index].astype('uint8'))\n",
    "        mod_img = self.transform(temp_img)\n",
    "\n",
    "        label = self.target_arr[index]        \n",
    "        return mod_img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_arr.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to be sure to randomize x_data and y_data, because the below only takes the last 3000 snapshots\n",
    "# of the track, which isn't ideal way to validate...\n",
    "train_data = CustomDataSet(x_data[:-3000], y_data[:-3000])\n",
    "valid_data = CustomDataSet(x_data[-3000:], y_data[-3000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes={}\n",
    "dataset_sizes['train'] = len(train_data)\n",
    "dataset_sizes['valid'] = len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoaders = {}\n",
    "dataLoaders['train'] = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=8)\n",
    "dataLoaders['valid'] = DataLoader(valid_data, batch_size=32, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in tqdm(dataLoaders[phase]):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                \n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(vgg16, criterion, optimizer, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
